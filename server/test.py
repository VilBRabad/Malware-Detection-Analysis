import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
# import sklearn
# print(sklearn.__version__)
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
from sklearn.manifold import TSNE
from sklearn.metrics import roc_auc_score

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import matthews_corrcoef, auc, precision_recall_curve, classification_report
import pickle

original_df = pd.read_csv('Obfuscated-MalMem2022.csv', sep=',', encoding='utf-8')
original_df.shape

df = original_df.copy()
df.shape

def select_every_nth_row(df, n=1000):
    selected_rows = df.iloc[::n]
    return selected_rows

selected_rows = select_every_nth_row(df, n=1000)

columns_to_drop = ['pslist.nprocs64bit', 'handles.nport', 'svcscan.interactive_process_services']

# Drop the specified columns
df.drop(columns=columns_to_drop, inplace=True)

df.fillna(method="ffill", inplace=True)  # Forward fill missing values
df.drop_duplicates(inplace=True)

df["Class"] = df["Class"].astype("category")

df = pd.get_dummies(df, columns=["Class"], drop_first=True)


# Separate features and target
y = df["Class_Malware"]
X = df.drop(columns=["Category", "Class_Malware"])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# Feature extraction using t-SNE
tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
X_tsne = tsne.fit_transform(X_scaled)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)
# print(X_test)
# XGBoost for Feature Extraction
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)

# Use the trained XGBoost model to get feature importances
feature_importances = xgb_model.feature_importances_

# Select the top N important features
N = 14  # Change this value based on the number of features you want to select
selected_feature_indices = np.argsort(feature_importances)[::-1][:N]
selected_features = X.columns[selected_feature_indices]

# Subset the dataset with the selected features
X_train_selected = X_train.iloc[:, selected_feature_indices]
X_test_selected = X_test.iloc[:, selected_feature_indices]

# Normalize the data
scaler = StandardScaler()
X_train_selected = scaler.fit_transform(X_train_selected)
X_test_selected = scaler.transform(X_test_selected)



#*******************************DNN+RNN

X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X_tsne, y.values, test_size=0.2, random_state=42)

# Convert the data to PyTorch tensors
X_train_tensor = torch.from_numpy(X_train).float()
y_train_tensor = torch.from_numpy(y_train_encoded).long()
X_test_tensor = torch.from_numpy(X_test).float()
y_test_tensor = torch.from_numpy(y_test_encoded).long()



class MalwareDetector(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MalwareDetector, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = x.unsqueeze(1)  # Add a time step dimension
        out = torch.relu(self.fc1(x))
        out = torch.relu(self.fc2(out))
        out, _ = self.rnn(out)
        out = self.fc3(out[:, -1, :])  # Get the last output of the RNN
        return out


input_size = len(X_train[0])
hidden_size = 128
output_size = 2  # 2 classes: benign and malicious
model = MalwareDetector(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)


#***************************** TRAINNING LOOP ******************************

# Initialize variables to store metrics
train_losses = []
train_accuracies = []
train_auc_scores = []
test_losses = []
test_accuracies = []
test_auc_scores = []

train_loss_sum = 0.0
num_correct_train = 0
num_samples_train = 0
y_true_train = []
y_pred_train = []

model_metrics = {}

model_metrics["DNN MalwareDetector"] = {}

# Define the number of epochs
epochs = 10

# Loop through epochs
for epoch in range(epochs):
    model.train()

    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Calculate training loss
        train_loss_sum += loss.item() * inputs.size(0)

        # Calculate number of correct predictions and total samples in the current batch
        _, predicted_train = torch.max(outputs.data, 1)
        num_correct_train += (predicted_train == labels).sum().item()
        num_samples_train += labels.size(0)

        # Store true labels and predicted labels for AUC calculation
        y_true_train.extend(labels.cpu().numpy())
        y_pred_train.extend(predicted_train.cpu().numpy())

    # Calculate training accuracy
    train_accuracy = 100 * num_correct_train / num_samples_train
    train_losses.append(train_loss_sum / num_samples_train)
    train_accuracies.append(train_accuracy)

    # Calculate AUC score for training data
    train_auc = roc_auc_score(y_true_train, y_pred_train, average='weighted')
    train_auc_scores.append(train_auc)

    # Evaluate the model on the test set
    model.eval()
    test_loss_sum = 0.0
    num_correct_test = 0
    num_samples_test = 0
    y_true_test = []
    y_pred_test = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            test_loss_sum += loss.item() * inputs.size(0)

            # Calculate number of correct predictions and total samples in the current batch
            _, predicted_test = torch.max(outputs.data, 1)
            num_correct_test += (predicted_test == labels).sum().item()
            num_samples_test += labels.size(0)

            # Store true labels and predicted labels for AUC calculation
            y_true_test.extend(labels.cpu().numpy())
            y_pred_test.extend(predicted_test.cpu().numpy())

    # Calculate test accuracy
    test_accuracy = 100 * num_correct_test / num_samples_test
    test_losses.append(test_loss_sum / num_samples_test)
    test_accuracies.append(test_accuracy)

    # Calculate AUC score for test data
    test_auc = roc_auc_score(y_true_test, y_pred_test, average='weighted')
    test_auc_scores.append(test_auc)

    # Print epoch-wise metrics
    print(f"Epoch [{epoch+1}/{epochs}] - "
          f"Train Loss: {train_losses[-1]:.4f}, "
          f"Train Accuracy: {train_accuracy:.2f}%, "
          f"Train AUC: {train_auc:.4f}, "
          f"Test Loss: {test_losses[-1]:.4f}, "
          f"Test Accuracy: {test_accuracy:.2f}%, "
          f"Test AUC: {test_auc:.4f}")

print("Training finished.")



# #**************************EVALUATION *************************8
# model.eval()
correct = 0
total = 0
y_true = []
y_pred = []

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Store true labels and predicted labels for evaluation
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate accuracy
accuracy = 100 * correct / total

# # Calculate additional evaluation metrics
# precision = precision_score(y_true, y_pred, average=None)
# recall = recall_score(y_true, y_pred, average=None)
# f1 = f1_score(y_true, y_pred, average=None)
# conf_matrix = confusion_matrix(y_true, y_pred)

# model_metrics["DNN MalwareDetector"]["Accuracy"] = accuracy
# model_metrics["DNN MalwareDetector"]["Precision"] = precision[0]
# model_metrics["DNN MalwareDetector"]["Recall"] = recall[0]
# model_metrics["DNN MalwareDetector"]["F1 Score"] = f1[0]
# model_metrics["DNN MalwareDetector"]["Confusion Matrix"] = conf_matrix

# # Calculate Sensitivity and Specificity
# sensitivity = recall[1]
# specificity = recall[0]

# # Calculate MCC
# mcc = matthews_corrcoef(y_true, y_pred)

# # Calculate AUC-PR
# precision, recall, _ = precision_recall_curve(y_true, y_pred)
# auc_pr = auc(recall, precision)



#**************************************LOGIISTIC*****************************
import torch.nn.functional as F

class LogisticRegression(nn.Module):
    def __init__(self, input_size, output_size):
        super(LogisticRegression, self).__init__()
        self.fc1 = nn.Linear(input_size, output_size)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        out = self.fc1(x)
        return F.softmax(out, dim=1)


#**************************************SVM*****************************
from sklearn.svm import SVC

# Create an SVM classifier
svm_classifier = SVC(kernel='linear', C=0.01)


#**************************************RFC*****************************
from sklearn.ensemble import RandomForestClassifier

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(
    n_estimators=50,  # Reduce the number of trees
    max_depth=5,      # Limit the depth of trees
    min_samples_split=2,  # Increase the minimum samples required to split
    min_samples_leaf=1,   # Increase the minimum samples required for a leaf
    random_state=42
)

N = 14  # Number of top features to select
sorted_indices = np.argsort(feature_importances)[::-1]
selected_features = X.columns[sorted_indices[:N]]




logistic_regression_model = LogisticRegression(input_size, output_size)

svm_classifier = SVC()
svm_classifier.fit(X_train, y_train_encoded)

rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)
rf_classifier.fit(X_train, y_train_encoded)

#gb_classifier = GradientBoostingClassifier()
#gb_classifier.fit(X_train, y_train_encoded)

# Train and evaluate models
models = {
    "Logistic Regression": logistic_regression_model,
    "SVM": svm_classifier,
    "Random Forest": rf_classifier,
    #"Gradient Boosting": gb_classifier,
}



for model_name, model in models.items():
    model_metrics[model_name] = {}

    if model_name == "Logistic Regression":
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        epochs = 10

        for epoch in range(epochs):
            optimizer.zero_grad()
            outputs = model(X_train_tensor)
            loss = criterion(outputs, y_train_tensor)
            loss.backward()
            optimizer.step()

        model.eval()
        with torch.no_grad():
            outputs = model(X_test_tensor)
            _, predicted = torch.max(outputs.data, 1)

            accuracy = accuracy_score(y_test_encoded, predicted)
            precision = precision_score(y_test_encoded, predicted)
            recall = recall_score(y_test_encoded, predicted)
            f1 = f1_score(y_test_encoded, predicted)
            conf_matrix = confusion_matrix(y_test_encoded, predicted)

        model_metrics[model_name]["Accuracy"] = accuracy
        model_metrics[model_name]["Precision"] = precision
        model_metrics[model_name]["Recall"] = recall
        model_metrics[model_name]["F1 Score"] = f1
        model_metrics[model_name]["Confusion Matrix"] = conf_matrix
    else:
        model.fit(X_train, y_train_encoded)
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test_encoded, y_pred)
        precision = precision_score(y_test_encoded, y_pred)
        recall = recall_score(y_test_encoded, y_pred)
        f1 = f1_score(y_test_encoded, y_pred)
        conf_matrix = confusion_matrix(y_test_encoded, y_pred)

        model_metrics[model_name]["Accuracy"] = accuracy
        model_metrics[model_name]["Precision"] = precision
        model_metrics[model_name]["Recall"] = recall
        model_metrics[model_name]["F1 Score"] = f1
        model_metrics[model_name]["Confusion Matrix"] = conf_matrix


# torch.save(model.state_dict(), 'model1.pth')

# # 2. Export the model to a pickle file
# with open('model1.pkl', 'wb') as f:
#     pickle.dump(model, f)

