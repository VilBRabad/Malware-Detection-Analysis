import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
# from sklearn.metrics import roc_auc_score
import torch
import torch.optim as optim
from Feature_Extraction import X_tsne, y
import pickle

# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
# from sklearn.metrics import matthews_corrcoef, auc, precision_recall_curve, classification_report

X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X_tsne, y.values, test_size=0.2, random_state=42)

# Convert the data to PyTorch tensors
X_train_tensor = torch.from_numpy(X_train).float()
y_train_tensor = torch.from_numpy(y_train_encoded).long()
X_test_tensor = torch.from_numpy(X_test).float()
y_test_tensor = torch.from_numpy(y_test_encoded).long()

input_size = len(X_train[0])
hidden_size = 128
output_size = 2


class LogisticRegression(nn.Module):
    def __init__(self, input_size, output_size):
        super(LogisticRegression, self).__init__()
        self.fc1 = nn.Linear(input_size, output_size)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        out = self.fc1(x)
        return F.softmax(out, dim=1)
    
model = LogisticRegression(input_size, output_size)


optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
epochs = 10

for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()


torch.save(model.state_dict(), 'logistic_pickel.pth')

# 2. Export the model to a pickle file
with open('logistic_pickel.pkl', 'wb') as f:
    pickle.dump(model, f)
