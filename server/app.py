from dotenv import load_dotenv
load_dotenv()

from flask import Flask, jsonify, request, session
from supabase import create_client
from flask_cors import CORS
from utils import *
import os
import bcrypt
import torch
import pickle
# from models.RNN_Model import MalwareDetector 
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import json
from sklearn.ensemble import RandomForestClassifier
from models.RNN_Model_def import MalwareDetector
from models.Logistic_Model_def import LogisticRegression
import joblib

app = Flask(__name__)
CORS(app)
app.secret_key = os.environ.get("SECRETE_KEY")

url = os.environ.get("SUPABASE_URL")
key = os.environ.get("SUPABASE_KEY")
supabase = create_client(url, key)


@app.route("/")
def home():
    return "Vilas"

#********************** User Auth ************************
@app.route("/sign-up", methods=['POST'])
def register():
    req = request.json
    print(req)
    try:
        if req:
            email = req.get("email")
            password = req.get("password")
            if email == "" or password == "":
                return jsonify({"status": 401, "error": "Email or password can not empty"})
            
            checkUser = supabase.table("users").select("*").eq("email", email).execute()
            if len(checkUser.data) != 0:
                return jsonify({"status": 401, "error": "Email id already exists!"})
            
            hashed = hashedPassword(password)
            # print(hashed)

            db_res = supabase.table("users").insert({"email": email, "password": hashed}).execute()
            if db_res:
                return jsonify({"status": 201, "message": "register successfully"})
            else:
                return jsonify({"status": 400, "error": "unable to register"}) 

    except Exception as err:
        return jsonify({"status": 500, "error": "Server Error!"})


@app.route("/sign-in", methods=['POST'])
def login():
    try:
        req = request.json
        if req:
            email = req.get("email")
            password = req.get("password")
            if email == "" or password == "":
                return jsonify({"status": 401, "error": "Email or password can not empty"})
            
            db_res = supabase.table("users").select("*").eq("email", email).execute()
            data = db_res.data
            if len(data) == 0:
                return jsonify({"status": 401, "error": "Wrong Credentials"})
            
            # print("Vilas", data[0]['password'])
            hased = data[0]['password']
            if CheckPassword(password, hased):
                print("VIlas", email)
                session['user'] = email
                return jsonify({"status": 201, "message": "login successfully", "user": email})
            else:
                return jsonify({"status": 400, "error": "Wrong Credentials"})
            
    except Exception as err:
        return jsonify({"status": 500, "error": "Server Error!"})


@app.route("/logout", methods=['POST'])
def logout():
    session.pop("user", None)
    return jsonify({"status": 201, "message": "Logout Successfully"})

@app.route("/is-login", methods=['POST'])
def isLogin():
    if "user" in session:
        user = session["user"]
        return jsonify({"status":200, "message": "user loged in", "user":user})
    return jsonify({"status":401, "error": "please login...."})


@app.route('/upload-file', methods=['POST'])
def uploadFile():
    try:
        csv_file = request.files['file']
        if csv_file.filename.endswith('.csv'):
            csv_file.save('uploaded_file.csv')

            original_df = pd.read_csv('uploaded_file.csv', sep=',', encoding='utf-8')
            original_df.shape

            #*************** Cleaning File *************************
            df = original_df.copy()
            df.shape

            def select_every_nth_row(df, n=1000):
                selected_rows = df.iloc[::n]
                return selected_rows

            selected_rows = select_every_nth_row(df, n=1000)

            columns_to_drop = ['pslist.nprocs64bit', 'handles.nport', 'svcscan.interactive_process_services']

            # Drop the specified columns
            df.drop(columns=columns_to_drop, inplace=True)

            df.fillna(method="ffill", inplace=True)  # Forward fill missing values
            df.drop_duplicates(inplace=True)

            df["Class"] = df["Class"].astype("category")

            df = pd.get_dummies(df, columns=["Class"], drop_first=True)


            #******************* Separate features and target
            y = df["Class_Malware"]
            X = df.drop(columns=["Category", "Class_Malware"])

            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)


            # Feature extraction using t-SNE
            tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
            X_tsne = tsne.fit_transform(X_scaled)

            X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X_tsne, y.values, test_size=0.2, random_state=42)

            input_size = len(X_train[0])

            X_train_tensor = torch.from_numpy(X_train).float()
            X_test_tensor = torch.from_numpy(X_test).float()
            y_test_tensor = torch.from_numpy(y_test_encoded).long()
            y_train_tensor = torch.from_numpy(y_train_encoded).long()

            test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)
            dnn_metrices = DNN_model(test_dataset, input_size)
            logistic_metrices = Logistic_Model(X_test_tensor, y_test_encoded, input_size)
            random_forest_metrices = random_Forest_model(X_train, y_train_encoded, X_test, y_test_encoded)

            data = {
                "DNN": dnn_metrices, 
                "LOG": logistic_metrices,
                "RFM": random_forest_metrices
            }
            for model_data in data.values():
                model_data['Confusion Matrix'] = model_data['Confusion Matrix'].tolist()

            # Convert the dictionary to JSON
            json_data = json.dumps(data)
            print(json_data)
            return jsonify(json_data)
        else:
            return jsonify({"status": 500, "error": 'not uploaded'})
    except Exception as err:
        print("Error: ", err)
        return jsonify({"status": 500, "error": "server error"})


def DNN_model(test_dataset, input_size):
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)


    hidden_size = 128
    output_size = 2
    model = MalwareDetector(input_size, hidden_size, output_size)

    # Load the model's parameters (state dictionary)
    model.load_state_dict(torch.load('dnn_pickel.pth'))

    correct = 0
    total = 0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # Store true labels and predicted labels for evaluation
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    # Calculate accuracy
    accuracy = 100 * correct / total

    # Calculate additional evaluation metrics
    precision = precision_score(y_true, y_pred, average=None)
    recall = recall_score(y_true, y_pred, average=None)
    f1 = f1_score(y_true, y_pred, average=None)
    conf_matrix = confusion_matrix(y_true, y_pred)

    dnn_metrices = {
        "Accuracy": accuracy,
        "Precision": precision[0],
        "Recall": recall[0],
        "F1 Score": f1[0],
        "Confusion Matrix": conf_matrix
    }
    print("\n***************DNN*******************")
    print("Accuracy: ", accuracy)
    print("Precision: ", precision[0])
    print("Recall: ", recall[0])
    print("F1 Score: ", f1[0])
    print("Confusion Matrix: ", conf_matrix)

    return dnn_metrices


def Logistic_Model(X_test_tensor, y_test_encoded, input_size):
    output_size = 2
    output_size = 2
    model = LogisticRegression(input_size, output_size)

    # Load the model's parameters (state dictionary)
    model.load_state_dict(torch.load('logistic_regression_state.pth'))

    model.eval()
    with torch.no_grad():
        outputs = model(X_test_tensor)
        _, predicted = torch.max(outputs.data, 1)

        accuracy = accuracy_score(y_test_encoded, predicted)*100
        precision = precision_score(y_test_encoded, predicted)
        recall = recall_score(y_test_encoded, predicted)
        f1 = f1_score(y_test_encoded, predicted)
        conf_matrix = confusion_matrix(y_test_encoded, predicted)

    print("\n***************Logistic*******************")
    print("Accuracy: ", accuracy)
    print("Precision: ", precision)
    print("Recall: ", recall)
    print("F1 Score: ", f1)
    print("Confusion Matrix: ", conf_matrix)

    logistic_metrices = {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1,
        "Confusion Matrix": conf_matrix
    }
    return logistic_metrices

def random_Forest_model(X_train, y_train_encoded, X_test, y_test_encoded):

    rf_classifier = joblib.load('rfc_model.pkl')

    y_pred = rf_classifier.predict(X_test)

    accuracy = accuracy_score(y_test_encoded, y_pred)*100
    precision = precision_score(y_test_encoded, y_pred)
    recall = recall_score(y_test_encoded, y_pred)
    f1 = f1_score(y_test_encoded, y_pred)
    conf_matrix = confusion_matrix(y_test_encoded, y_pred)

    metrices = {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1,
        "Confusion Matrix": conf_matrix
    }
    return metrices

if __name__ == '__main__':
    app.run(debug=True)